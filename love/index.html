<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">

		<title>LoVe Guest Lecture: Getting Assistance from Proof Assistants</title>

		<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.2.1/reveal.min.css">
        <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.3/css/all.css">
		<link rel="stylesheet" href="plugin/chalkboard.css">
		<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.2.1/theme/white.min.css">
        <link rel="preconnect" href="https://fonts.gstatic.com">
        <link href="https://fonts.googleapis.com/css2?family=Lato&family=Roboto&family=Roboto+Mono&family=Share+Tech+Mono&display=swap" rel="stylesheet">


        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlightjs@latest/styles/github.css">
        <style type="text/css">
            h2, h3 { font-family: 'Lato', sans-serif !important; text-transform: none !important; }
            p { font-family: 'Roboto', sans-serif !important; }
            p { text-align: left;}
            section {font-size: 0.90em !important;}
            code {font-family: 'Share Tech Mono', monospace !important; max-height: 500px !important;}
            pre {font-size: 0.7em !important;}
        </style>
	</head>

	<body>

		<div class="reveal">

			<div class="slides">

                <!-- Use external markdown resource, separate slides by three newlines; vertical slides by two newlines 
                <section data-markdown="markdown.md" data-separator="^\n\n\n" data-separator-vertical="^\n\n"></section> -->

                <!-- Slides are separated by three dashes (quick 'n dirty regular expression) -->
                <section data-markdown data-separator="^---">
                    <script type="text/template">
## Getting Assistance from Proof Assistants
### Alex J. Best - Vrije Universiteit Amsterdam
#### LoVe Guest Lecture 2021



These slides are online at:

> <http://alexjbest.github.io/talks/love/>

---
## Who am I?

- Researcher in the mathematics department
- Mathematical research is in number theory / arithmetic geometry
- Interested in computation, algorithms to calculate interesting things in my area of mathematics, but also more generally
- Have been interested in formalization for a few years, first as a hobby, but now part of my work


## Motivation

*Why formalize advanced mathematics?*

My long-term hope is that formalization / verification will enable more effective research.
No more skimming papers for relevant material, just `library_search`, easy access to a consistent reference work with all definitions consistently stated, and available at the click of a button.

But why might you care?

---
### Example 1: Integer Multiplication

What is the bit complexity of multiplying extremely large integers? From [Wikipedia](https://en.wikipedia.org/wiki/F%C3%BCrer%27s_algorithm):
> In 2015 Harvey, Joris van der Hoeven and Lecerf gave a new algorithm that achieves a running time of $ O(n\log n\cdot 2^{3\log ^{\star}n})$ making explicit the implied constant in the $O(\log ^{\star}n)$ exponent. They also proposed a variant of their algorithm which achieves ${\displaystyle O(n\log n\cdot 2^{2\log ^{\star}n})}$ but whose validity relies on standard conjectures about the distribution of [Mersenne primes](https://leanprover-community.github.io/mathlib_docs/number_theory/lucas_lehmer.html#mersenne).
> 
> In 2016 Covanov and Thom√© proposed an integer multiplication algorithm based on a generalization of Fermat primes(?) that conjecturally achieves a complexity bound of $ O(n\log n\cdot 2^{2\log ^{\star}n}).$ This matches the 2015 conditional result of Harvey, van der Hoeven, and Lecerf but uses a different algorithm and relies on a different conjecture.
> 
> In 2018 Harvey and van der Hoeven used an approach based on the existence of short lattice vectors guaranteed by [Minkowski's theorem](https://github.com/leanprover-community/mathlib/pull/2819/files#diff-56116a843d3b12ccaf46e2216c930c944d42ce0c73939716c965aea7a734d180R484) to prove an unconditional complexity bound of $O(n\log n\cdot 2^{2\log ^{\star}n})$.
> 
> In March 2019, Harvey and van der Hoeven published a long-sought after $O(n\log n)$ integer multiplication algorithm, which is conjectured to be asymptotically optimal. Because Sch√∂nhage and Strassen predicted that n log(n) is the ‚Äòbest possible‚Äô result Harvey said: ‚Äú...our work is expected to be the end of the road for this problem, although we don't know yet how to prove this rigorously.‚Äù
<!-- .element style="font-size:0.7em;" -->

Asymptotically optimal integer multiplication is based on high-level maths.

---
### Example 2: Matrix Multiplication

How many ring operations do you need to multiply large $n \times n$ matrices?

What is the minimum $\omega$ so that this can be done in $O(n^\omega)$?

Strassen shows $\omega < 2.81$, later work goes down to $\omega < 2.372864$.

Some conjecture $\omega = 2$!

Most successful  approach so far relates this problem to a group ring $\mathbf C[G]$ for a finite group $G$.

In 2016 Ellenberg-Gijswijt proved the Cap-Set conjecture, which in turn implies that this method with $G$ an abelian group cannot prove $\omega = 2$.

In 2019 Sander R. Dahmen, Johannes H√∂lzl, and Robert Y. Lewis [formalized the Ellenberg-Gijswijt result](https://github.com/lean-forward/cap_set_problem) in Lean.

References:
<https://arxiv.org/abs/1712.02302> <https://arxiv.org/abs/1605.06702>
<https://drops.dagstuhl.de/opus/volltexte/2019/11070/> <https://arxiv.org/abs/1605.09223>

---
### Example 3: Cryptography

Modern cryptography is based on turning difficult mathematical problems into methods of encryption.

E.g. factoring, elliptic curves, isogenies, lattices.

Internet security is therefore based extensively on the hardness of problems in these areas.

When choosing parameters we want to be sure that no known attack works.

When implementing the mathematical parts of these cryptographic algorithms we want to be sure the implementation is correct in all cases.

---
### Example 4: the Sensitivity conjecture

From [Scott Aaronson's blog](https://www.scottaaronson.com/blog/?p=4229):
> The Sensitivity Conjecture, says that, for every Boolean function $f:\\{0,1\\}^n\to \\{0,1\\}$, the sensitivity of $f$‚Äîthat is, the maximum, over all $2^n$ input strings $x\in \\{0,1\\}^n$, of the number of input bits such that flipping them changes the value of $f$‚Äîis at most polynomially smaller than a bunch of other complexity measures of $f$, including $f$‚Äôs block sensitivity, degree as a real polynomial, and classical and quantum query complexities.

This was proved in a [stunningly short 6 page paper](https://arxiv.org/abs/1907.00847) by Hao Huang in 2019, and the proof was formalized in Lean within a couple of months by Reid Barton, Johan Commelin, Jesse Han, Chris Hughes, Robert Y. Lewis, Patrick Massot.
[github link](https://github.com/leanprover-community/mathlib/blob/master/archive/sensitivity.lean)
---


## How can we make formalizing easier / less time consuming?

*Formalization is still hard*, harder than writing paper proofs down, at least.

How can we reduce this:

1. Don't reinvent the wheel!
1. Create and use high level tactics to speed through conceptually simple proofs
1. Create and use tools to make repetitive or automatable tasks easier

---
### Don't reinvent the wheel
Mix and match code reuse is harder when formalizing your code. Consider building on existing foundations instead:
![monolith](monolith.jpg) <!-- .element style="display: block; width: 40%;margin-left: auto; margin-right: auto; margin-top:0;margin-bot:0;" -->
The ‚ÄúMonolith‚Äù, by Jean Nouvel ‚Äì a steel cube with an edge length of 34 feet ‚Äì Expo 02 Arteplage in Morat

---
### Comparing approaches

<nbsp/> | pros | cons
 ---- | ------ | -------- 
starting from scratch | complete independence, not forced into any implementation  | have to reinvent the wheel
building on a monolith / other libraries | lots of things for free | breakage if the other libraries change, incompatible versions, nobody will fix your code for you
living in a monolith | many results already there, everything is almost guarenteed to interact nicely | unwieldy project size, extra tools needed to deal with this, higher standards for code, many people with conflicting opinions on the way things should be done

The community of users of Lean has tended towards investing the most effort into a monolith (`mathlib`).

Other projects do exist that build on top of `mathlib`.

---
## What is `mathlib`?

- Collaborative open-source library of Lean (3) formalizations
- 30,000 definitions, 71,000 theorems

- 10-20 PRs of new material merged per day, some big some small

- Over 200 contributors so far, from a huge range of people

- 2,200 files, covering Algebra, Analysis, Geometry, Logic, Topology, Probability, some computability theory and combinatorics, basic utilities for lists, strings and other programming essentials.

[Open demo project](vscode://file/Users/alex/talks/love/) - [Import graph](https://alexjbest.github.io/mathlib-import-graph/?highlight=core:data.dlist&docs_url=https://leanprover-community.github.io/mathlib_docs/) by Eric Wieser

But there are other systems out there with libraries too, why not build on those?

Translation between systems is difficult.
---
## Translating between / within proof assistants

- Metamath is a more bare-bones proof checker than Lean, that contains a large body of mathematics based on ZFC set theory (not type theory).

  Mario Carneiro has implemented a translator (`mm0-lean`) that embeds such proofs in Lean.
Using this it is possible to transfer results such as Dirichlet's theorem on primes in arithmetic progressions, or the prime number theorem into valid Lean proofs based on those in Metamath.

  So: Translating correct proofs from one logic/implementation to anther is possible, but actually using the output is far harder. It is usable if a single theorem about simple objects is the desired result, but aligning larger types or whole theories is not simple.

- The project of porting the entire `mathlib` library from Lean 3 to Lean 4 is currently underway, although the implementation and logic of these systems are very similar porting the library successfully means producing reasonable looking Lean 4 source code, rather than just transferring results.

  Even within one system automatically producing modifications of existing results is useful.
  E.g. `to_additive`.

---
## Powerful tactics

You have (possibly) already seen:
- `finish`/`tauto`
- `(n)linarith` - [geometry.manifold.instances](vscode://file/Users/alex/talks/love/_target/deps/mathlib/src/geometry/manifold/instances/sphere.lean:203:5)
- `library_search`
- `abel`
- `norm_cast`
- `ring` - [algebra.group_power.identities](vscode://file/Users/alex/talks/love/_target/deps/mathlib/src/algebra/group_power/identities.lean:62:3)

Some others:
- `suggest using` - [demo](vscode://file/Users/alex/talks/love/src/demo_suggest_using.lean:9:3)
- `group`/`noncomm_ring` - [demo](vscode://file/Users/alex/talks/love/src/demo_group.lean:164:3)
- `norm_num` - Normalizes numerical expressions in any ring
- `obviously` / `tidy` - Apply other tactics in a chain, most useful for proofs that have a particular style, used heavily in categorical machinery
- `rewrite_search` - [tests](vscode://file/Users/alex/talks/love/_target/deps/mathlib/test/rewrite_search/rewrite_search.lean:59:1)

Most of these have specific uses and purposes, this is good, but general purpose automation would be better!
---
## General purpose automation, aka magic button that does my work for me

In the proof assistant `Isabelle` there is good integration with external automated theorem provers (ATPs), called Sledgehammer, due to Jasmin!

As this functions by calling an external program to find the proof there are two stages, translation into the external logic and format, and reconstruction of the resulting proof based on the output of the ATP.

Isabelle has a very different logical foundation to `Lean`, so exactly the same approach will not have the same successes.

The proof assistant `Coq` which is similar in many respects to `Lean` has a version of this called `CoqHammer`, it looks quite effective.

In `Lean` and `mathlib` there have been some promising experiments, but nothing concrete has landed... yet!

---
## GPT-f

Jesse Michael Han, Jason Rute, Yuhuai Wu, Edward W. Ayers and Stanislas Polu, supported by the company Open-AI have built a transformer language model to predict tactic calls.

By applying machine learning trained on Mathlib, other mathematical and scientific literature on the internet, and large text corpuses they achieve pretty incredible results.

[Demo](vscode://file/Users/alex/talks/love/src/demo_group.lean:162:3)

<https://arxiv.org/abs/2102.06203>

---
## Linters

If the proof assistant can not only helping us verify proofs, but also check that our results are as useful as possible, we can work more effectively and reduce the human work involved.

If we can think less about the small stuff we can spend more time on the big issues!

Let's talk about several ways we can do this, in some examples.

I'll mention about a few ways we can use meta-programming in Lean to inspect the entirety of a large library and suggest improvements and help fix oversights.

We can either run these on whole of mathlib, or in an individual file we are interested in by typing `#lint`.
---
### Unused arguments
What's wrong with the following?
```lean
variables {A S : Type*} [comm_ring A] [is_domain A] [comm_ring S]
variables {M : submonoid A} [algebra A S] [is_localization M S]

lemma scale_roots_aeval_eq_zero_of_aeval_mk'_eq_zero {p : polynomial A} {r : A} {s : M}
  (hr : aeval (mk' S r s) p = 0) :
  aeval (algebra_map A S r) (scale_roots p s) = 0 :=
begin
  convert scale_roots_eval‚ÇÇ_eq_zero (algebra_map A S) hr,
  rw [aeval_def, mk'_spec' _ r s]
end
```
The fact `[is_domain A]` is never used in the proof, this is essentially impossible to tell just by looking at the proof on a slide! Only way to work this out is by inspecting the proof step by step, or thinking about what generality the lemma should be true in, waste of time!

This was originally written by Floris van Doorn.


---
### Syntactic tautologies
What is wrong with the following definition?
```
/-- The identity linear isometry. -/
def id : E ‚Üí‚Çó·µ¢[R] E := ‚ü®linear_map.id, Œª x, rfl‚ü©

@[simp] lemma coe_id : ‚áë(id : E ‚Üí‚Çó·µ¢[R] E) = id := rfl
```

### Duplicate lemmas
Lemmas can be duplicates of each other with different proofs, most of the time this is unintentional.

By hashing the types of lemmas and throwing them into a dictionary we can quickly find these.

There were around 100 pairs of duplicates in mathlib last time I checked, many due to copy-paste errors:
```
lemma differentiable_at_cosh {x : ‚ÑÇ} : differentiable_at ‚ÑÇ cos x :=
differentiable_cos x
```

---
### Unused haves / suffices
If we have a proof using
```
have h : some_fact := <proof_of_fact>,
<rest_of_proof>
```
or
```
suffices h : some_fact,
<rest_of_proof>,
<proof_of_fact>
```
and where `rest_of_proof` doesn't make use of the hypothesis `h`, then introducing and proving `h` in this way was unnecessary.
Such a proof looks like this under the hood.

```
lemma a : true :=
have 1 = 1 := rfl,
trivial

lemma b : true :=
(Œª h : 1 = 1, trivial) rfl
```

By looping over all the produced proofs in the environment we can find instances of this.

---

Such as
```
lemma mem_traverse {f : Œ±' ‚Üí set Œ≤'} :
  ‚àÄ(l : list Œ±') (n : list Œ≤'), n ‚àà traverse f l ‚Üî forall‚ÇÇ (Œªb a, b ‚àà f a) n l
| []      []      := by simp
| (a::as) []      := by simp; exact assume h, match h with end
| []      (b::bs) := by simp
| (a::as) (b::bs) :=
  suffices (b :: bs : list Œ≤') ‚àà traverse f (a :: as) ‚Üî b ‚àà f a ‚àß bs ‚àà traverse f as,
    by simp [mem_traverse as bs],
  iff.intro
    (assume ‚ü®_, ‚ü®b, hb, rfl‚ü©, _, hl, rfl‚ü©, ‚ü®hb, hl‚ü©)
    (assume ‚ü®hb, hl‚ü©, ‚ü®_, ‚ü®b, hb, rfl‚ü©, _, hl, rfl‚ü©)
```
can become
```
lemma mem_traverse {f : Œ±' ‚Üí set Œ≤'} :
  ‚àÄ(l : list Œ±') (n : list Œ≤'), n ‚àà traverse f l ‚Üî forall‚ÇÇ (Œªb a, b ‚àà f a) n l
| []      []      := by simp
| (a::as) []      := by simp
| []      (b::bs) := by simp
| (a::as) (b::bs) := by simp [mem_traverse as bs]
````

Simpler proofs are easier to understand (for both humans and computers!), easier fix when they break, and easier generalize / copy to different contexts.

---
### Unnecessary case splits
Similarly if we begin make use of a split into cases in a proof, but then don't use the hypothesis it is unnecessary
```
lemma uniformly_extend_spec {Œ± Œ≤ Œ≥ : Type*} [uniform_space Œ±] [uniform_space Œ≤] [uniform_space Œ≥]
  {e : Œ≤ ‚Üí Œ±} (h_e : uniform_inducing e) (h_dense : dense_range e) {f : Œ≤ ‚Üí Œ≥}
  (h_f : uniform_continuous f) [separated_space Œ≥] [complete_space Œ≥] (a : Œ±) :
  tendsto f (comap e (ùìù a)) (ùìù (œà a)) :=
let de := (h_e.dense_inducing h_dense) in
begin
  by_cases ha : a ‚àà range e,
  { rcases ha with ‚ü®b, rfl‚ü©,
    rw [uniformly_extend_of_ind _ _ h_f, ‚Üê de.nhds_eq_comap],
    exact h_f.continuous.tendsto _ },
  { simp only [dense_inducing.extend, dif_neg ha],
    exact tendsto_nhds_lim (uniformly_extend_exists h_e h_dense h_f _) }
end
```
in this example `ha` is only used in the first branch, and when we remove the split, it turns out the hypothesis `[separated_space Œ≥]` is no longer needed.
The proof becomes
```
by simpa only [dense_inducing.extend] using tendsto_nhds_lim (uniformly_extend_exists h_e ‚Äπ_‚Ä∫ h_f _)
```
and we can consequently remove the `separated_space` assumption from 10 other lemmas in the library.

We just avoided someone potentially having to do this by hand later.

---
### Removable edge cases
So far we have talked a lot about simplifying proofs, what about simplifying statements?

One common and annoying way statements can be sub-optimal is that they have conditions about edge cases that need verifying.
This is one of the things that makes formal proof more time consuming than informal proof.

The following is an example of a statement about a `group` `G`.
```
lemma exponent_dvd_of_forall_pow_eq_one (n : ‚Ñï) (hpos : 0 < n) (hG : ‚àÄ g : G, g ^ n = 1) :
  exponent G ‚à£ n :=
begin
  apply nat.dvd_of_mod_eq_zero,
  by_contradiction h,
  have h‚ÇÅ := nat.pos_of_ne_zero h,
  have h‚ÇÇ : n % exponent G < exponent G := nat.mod_lt _ (exponent_pos_of_exists _ n hpos hG),
  have h‚ÇÉ : exponent G ‚â§ n % exponent G,
  { apply exponent_min' _ _ h‚ÇÅ,
    simp_rw ‚Üêpow_eq_mod_exponent,
    exact hG },
  linarith,
end
```
---
By changing the type from
```
lemma exponent_dvd_of_forall_pow_eq_one (n : ‚Ñï) (hpos : 0 < n) (hG : ‚àÄ g : G, g ^ n = 1) : exponent G ‚à£ n
```
to
```
lemma exponent_dvd_of_forall_pow_eq_one (n : ‚Ñï) (hG : ‚àÄ g : G, g ^ n = 1) : exponent G ‚à£ n
```
the whole first half of the following proof in the same file becomes unnecessary:
```
lemma lcm_order_eq_exponent {H : Type*} [fintype H] [left_cancel_monoid H] :
  (finset.univ : finset H).lcm order_of = exponent H :=
begin
  apply nat.dvd_antisymm (lcm_order_of_dvd_exponent H),
  apply exponent_dvd_of_forall_pow_eq_one,
  { apply nat.pos_of_ne_zero,
    by_contradiction,
    rw finset.lcm_eq_zero_iff at h,
    cases h with g hg,
    simp only [true_and, set.mem_univ, finset.coe_univ] at hg,
    exact ne_of_gt (order_of_pos g) hg },
  { intro g,
    have h : (order_of g) ‚à£ (finset.univ : finset H).lcm order_of,
    { apply finset.dvd_lcm,
      exact finset.mem_univ g },
    cases h with m hm,
    rw [hm, pow_mul, pow_order_of_eq_one, one_pow] },
end
```
---
## Removing imports

Check out the [Import graph](https://alexjbest.github.io/mathlib-import-graph/?highlight=core:data.dlist&docs_url=https://leanprover-community.github.io/mathlib_docs/) again

- Each file imports many others that contain results that are then built on.

- Having a lot of things imported is great when writing a file

- But when we have a project on the scale of mathlib, where rechecking the library from scratch takes over 3 hours the more dependencies a file has, the more work needs to be done to rebuild when the file is changed.

For example `number_theory.class_number.admissible_abs` is a file about an inherently algebraic notion which somehow ended up importing `analysis.special_functions.pow` and therefore a whole bunch of analysis.

Me and Johan Commelin wrote a metaprogram in Lean to find which declarations are actually needed for each Lean file, and minimise the set of imports to only those files.

To do this we construct a directed acylic graph of all files, and have to find a minimal set of nodes such that the subgraph of their descendents still contains all the material needed for a given file.

---
### Generalizing typeclasses


**Problem**: Often theorems are stated with stronger, but still used, typeclass assumptions than are really needed, can this be detected automatically?

**A fake Lean lemma**:
```lean
lemma mul_inv {G : Type*} [ordered_comm_group G] (a b : G) : (a * b)‚Åª¬π = a‚Åª¬π * b‚Åª¬π :=
by rw [mul_inv_rev, mul_comm]
```

This assumes `[ordered_comm_group G]` but makes no mention of an order! In principle the proof could still require it, but it doesn't.

Anyone trying to use this theorem for a  `comm_group G` would get an error! <br />

The "correct" assumption is `[comm_group G]`,
when this is  changed the proof script does not need modifying.

**A real lemma from `mathlib`**:
```lean
lemma ring_hom.char_p_iff_char_p {K L : Type*} [field K] [field L]
(f : K ‚Üí+* L) (p : ‚Ñï) : char_p K p ‚Üî char_p L p :=
begin
  split;
  { introI _c, constructor, intro n,
    rw [‚Üê @char_p.cast_eq_zero_iff _ _ p _c n, ‚Üê f.injective.eq_iff, f.map_nat_cast, f.map_zero] }
end
```
Correct assumptions: `[division_ring K] [nontrivial L] [semiring L]`

---
## How do typeclasses work?
Lean automatically generates the terms used in the proof:
```lean
@ordered_comm_group.to_ordered_cancel_comm_monoid G _inst_1
@ordered_cancel_comm_monoid.to_ordered_comm_monoid ...
@ordered_comm_group.to_comm_group G _inst_1
...
```
via typeclass resolution. The following instances appear in the proof term:
 <!-- .element style="font-size:0.9em;" -->
![tc graph](tcchain.svg) <!-- .element style="display: block; width: 50%;margin-left: auto; margin-right: auto; margin-top:0;margin-bot:0;" -->

Typeclasses are used when the structure being inferred  is (locally) unique.

<!--The numerous steps that typeclass resolution can take for you are a big part of making Lean usable for mathematics, but it does make it easy to forget exactly what you are assuming or need for each lemma.-->
---
## What does the `mathlib` typeclass graph actually look like?

See https://observablehq.com/@alexjbest2/lean-generalisation for a (small) sample:
 <!-- .element style="font-size:0.8em;" -->
![tc graph](svg.svg) <!-- .element style="display: block; width: 80%;margin-left: auto; margin-right: auto; object-fit: cover; margin-top:-50px;margin-bot:0;" -->

(*) Only terms reachable from `linear_ordered_field` here, generated Jan 2021, it has grown since then!
 <!-- .element style="font-size:0.7em;" -->
---
## Why is detecting this reasonable?

- Because the aim is only to change the typeclass assumptions to match those actually used in the proof, the proof script will (likely) not need changing (*).
- The linter shouldn't need to do any nontrivial proving itself, so it should be comparatively fast.

A metaprogram which attempts to weaken assumptions and then fix the proofs would need to be far more involved!

<!--Is it worth the effort? While working on this I have noticed many many examples flagged by my prototype being independently changed by mathlib contributors (generalisations or complete refactors), anecdotally at least some things real people care about are noticed by the linter.-->

(*) Its possible to write a long proof term by hand that looks like it was generated by typeclass inference
 <!-- .element style="font-size:0.7em;" -->


## What are the most general typeclass assumptions?
<!-- .element: class="fragment" data-fragment-index="2" -->

- Looking at the proof term, we can determine the instances actually used to invoke the lemmas used.
<!-- .element: class="fragment" data-fragment-index="2" -->
- Then study the global typeclass graph to determine minimal assumptions providing those instances.
<!-- .element: class="fragment" data-fragment-index="2" -->
- Uniqueness is important, we must not introduce unequal instances of one typeclass, but we may want multiple assumptions with a common instance, if they will be guaranteed equal.
<!-- .element: class="fragment" data-fragment-index="2" -->
- To do this we take meets in the typeclass graph of sets of "conflicting" typeclasses (i.e. those which give instances of the same non-subsingleton typeclass).
<!-- .element: class="fragment" data-fragment-index="2" -->


---
## Caveats

- Just looking at the typeclasses defined in the library is not enough.

    Partially applied typeclasses are also important.

    Want to generalize `[group G]` to `[has_pow G ‚Ñ§]` if possible or `monoid G` to `has_pow G ‚Ñï`, so we treat these partially applied typeclasses as the basic objects of interest.

    This increases the complexity of the tool!

- <!-- .element: class="fragment" data-fragment-index="2" -->It is easy to "fool" such a tool:
    
    ```lean
    /-- Given a monoid hom `f : M ‚Üí* N` and submonoid `S ‚äÜ M` such that
    `f(S) ‚äÜ units N`, for all `w : M, z : N` and `y ‚àà S`, we have
    `z = w * (f y)‚Åª¬π ‚Üî z * f y = w`. -/
    lemma submonoid.localization_map.mul_inv_right {M : Type*}
    [comm_monoid M] {S : submonoid M} {N : Type*} [comm_monoid N]
    {f : M ‚Üí* N} (h : ‚àÄ y : S, is_unit (f y)) (y : S) (w z) :
    z = w * ‚Üë(is_unit.lift_right (f.mrestrict S) h y)‚Åª¬π ‚Üî
    z * f y = w := by rw [eq_comm, mul_inv_left h, mul_comm, eq_comm]
    ```
    the use of `mul_comm` here isn't actually needed, but its inclusion is enough to make it seem like `[comm_monoid M]` is required.

<!--- Stating each lemma in as much generality as possible may result in unnecessary complication in the library, and maybe even slow down some proofs if typeclass resolution needs to do more work.-->

---
## Implementation

This is implemented in Lean (no external dependencies):

> https://github.com/alexjbest/lean-generalisation

- Implemented using the linter framework, just import the tool and add `#lint` after your declaration.
- Uses `native.rb_lmap` to represent DAGs, find reachable sets and topological sorts - untrusted, nothing can be proven about this implementation! 
- Caches the typeclass graph and useful associated data.
- Now quick enough for interactive use in _most_ files.

### Some technical gotchas

- Must inspect the statement as well as the proof! Sometimes the proof is "just" `rfl`
- Some things look like generalisations but are mathematical no-ops (solution: keep a list of bad type synonyms)
- Sometimes terms need to be eta reduced (e.g. TC mechanism puts the term `(Œª a b, _inst_1 a b)` in the proof instead of `_inst_1`)

---
### Example output:
```lean
/- The `generalisation_linter` linter reports: -/
/- typeclass generalisations may be possible -/
-- topology\algebra\group.lean
#print nhds_translation_mul_inv /- _inst_3: topological_group ‚Üù has_continuous_mul
 -/
#print nhds_translation_add_neg /- _inst_3: topological_add_group ‚Üù has_continuous_add
 -/
#print quotient_add_group.is_open_map_coe /- _inst_3: topological_add_group ‚Üù has_continuous_add
 -/
#print quotient_group.is_open_map_coe /- _inst_3: topological_group ‚Üù has_continuous_mul
 -/
#print is_open.add_left /- _inst_3: topological_add_group ‚Üù has_continuous_add
 -/
#print is_open.mul_left /- _inst_3: topological_group ‚Üù has_continuous_mul
 -/
#print is_open.add_right /- _inst_3: topological_add_group ‚Üù has_continuous_add
 -/
#print is_open.mul_right /- _inst_3: topological_group ‚Üù has_continuous_mul
 -/
#print topological_group.t1_space /- _inst_3: topological_group ‚Üù has_continuous_mul
 -/
#print compact_open_separated_mul /- _inst_3: topological_group ‚Üù has_continuous_mul
 -/
#print compact_open_separated_add /- _inst_3: topological_add_group ‚Üù has_continuous_add
 -/
#print compact_covered_by_mul_left_translates /- _inst_3: topological_group ‚Üù has_continuous_mul
 -/
#print compact_covered_by_add_left_translates /- _inst_3: topological_add_group ‚Üù has_continuous_add
 -/
````
This is naturally an iterative process: once the assumptions lemmas are changed dependent declarations may become generalizable.
- The current implementation is fast enough for interactive use.
- Alternative would be to transform declarations in place when working through the environment
---
### Some common replacements in `mathlib`

Running a single pass of this on my laptop on the 80,000 declarations in  `mathlib` finished overnight.

Original typeclass | Replacement typeclasses (number of times replaced)
 ----- | ----
comm_ring | comm_semiring (42), ring (40), semiring (27), has_zero (8)
add_comm_group | add_comm_monoid (96), add_group (5), sub_neg_monoid (5), {has_add, has_neg, has_zero} (3)
semiring | non_assoc_semiring (53), non_unital_non_assoc_semiring (23), {add_comm_semigroup, has_one} (13), {add_comm_monoid, has_mul} (8), has_mul (7), has_zero (5)
field | division_ring (23), comm_ring (12), integral_domain (12), semiring (7), domain (4), ring (4), has_inv ring (3)
ring | semiring (55), non_assoc_semiring (8), {add_group, has_mul} (4), {has_add, has_neg, mul_zero_one_class} (4)
preorder | has_lt (36), has_le (31), {has_le, is_refl} (3), {has_lt, is_asymm} (3), {has_lt, is_irrefl, is_trans} (3)
comm_semiring | semiring (26), monoid (10), add_zero_class (4), {has_mul, is_associative, is_commutative} (4), has_pow (4), mul_one_class (4)
normed_space | module (23), semi_normed_space (38)
add_monoid | add_zero_class (51), {has_add, has_zero} (5)
monoid | mul_one_class (34), has_mul (11), {has_mul, has_one} (5), has_pow (5)
module | has_scalar (20), distrib_mul_action (8), mul_action (6)
normed_group | semi_normed_group (36), has_norm (10), has_nnnorm (3)
integral_domain | comm_ring (15), domain (8), {comm_ring, no_zero_divisors} (7), comm_monoid (3), {has_mul, has_zero, no_zero_divisors} (2), {no_zero_divisors, semiring} (2)
partial_order | preorder (33)
<!-- .element style="font-size:0.65em;" -->

---
## Other systems

Typeclasses are used in a similar way in Coq, Isabelle/HOL, Haskell, ...

Some related, but different work:
- "Eliciting Implicit Assumptions of Mizar Proofs by Property Omission" by Jesse Alama
- "Generalization in Type Theory Based Proof Assistants" by Olivier Pons


---
## Thanks for listening!

![tc graph](svg.svg) <!-- .element style="display: block; width: 60%;margin-left: auto; margin-right: auto; margin-top:0;margin-bot:0;" -->
> https://github.com/alexjbest/lean-generalisation
> https://github.com/alexjbest/dag-tools

                    </script>
                </section>

            </div>
		</div>

		<script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.2.1/reveal.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.2.1/plugin/markdown/markdown.js"></script>
        <script src="plugin/chalkboard.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.2.1/plugin/highlight/highlight.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.2.1/plugin/notes/notes.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.2.1/plugin/math/math.js"></script>
        <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@latest/build/highlight.min.js"></script>


		<script>

			Reveal.initialize({
                width: 1600,
                height: 900,
				controls: false,
				progress: false,
				history: true,
				center: false,
				previewLinks: false,
                transition: 'none',
                chalkboard: {
                    toggleChalkboardButton: false,
                    toggleNotesButton: false,},
				plugins: [ RevealMarkdown, RevealNotes, RevealChalkboard, RevealMath.MathJax3 ]
			});
            Reveal.addEventListener('ready', (event) => {
                hljs.registerLanguage("lean",function(){"use strict";return function(hljs){
  var LEAN_KEYWORDS = {
    keyword:
      'theorem|10 lemma|10 definition def class structure instance ' +
      'example inductive coinductive ' +
      'axiom axioms hypothesis constant constants ' +
      'universe universes variable variables parameter parameters ' +
      'begin end ' +
      'import open theory prelude renaming hiding exposing ' +
      'calc  match do  by let in extends ' +
      'fun assume ' +
      '#check #eval #reduce #print',
    built_in:
      'Type Prop|10 Sort rw|10 rewrite rwa erw subst substs ' +
      'simp dsimp simpa simp_intros finish ' +
      'unfold unfold1 dunfold unfold_projs unfold_coes ' +
      'delta cc ac_reflexivity ac_refl ' +
      'existsi|10 cases rcases with intro intros introv by_cases ' +
      'refl rfl funext propext exact exacts ' +
      'refine apply eapply fapply apply_with apply_instance ' +
      'induction rename assumption revert generalize specialize clear ' +
      'contradiction by_contradiction by_contra trivial exfalso ' +
      'symmetry transitivity destruct constructor econstructor ' +
      'left right split injection injections ' +
      'repeat try continue skip swap solve1 abstract all_goals any_goals done ' +
      'fail_if_success success_if_fail guard_target guard_hyp ' +
      'have replace at suffices show from ' +
      'congr congr_n congr_arg norm_num ring ',
    literal:
      'tt ff',
    meta:
      'noncomputable|10 private protected meta mutual',
    section:
      'section namespace',
    strong:
      'sorry admit',
  };

  var LEAN_IDENT_RE = /[A-Za-z_][\\w\u207F-\u209C\u1D62-\u1D6A\u2079\']*/;

  var DASH_COMMENT = hljs.COMMENT('--', '$');
  var MULTI_LINE_COMMENT = hljs.COMMENT('/-[^-]', '-/');
  var DOC_COMMENT = {
    className: 'doctag',
    begin: '/-[-!]',
    end: '-/'
  };

  var ATTRIBUTE_DECORATOR = {
    className: 'meta',
    begin: '@\\[',
    end: '\\]'
  };

  var ATTRIBUTE_LINE = {
    className: 'meta',
    begin: '^attribute',
    end: '$'
  };

  var LEAN_DEFINITION =	{
    className: 'theorem',
    beginKeywords: 'def theorem lemma class instance structure',
    end: ':=',
    excludeEnd: true,
    contains: [
      {
        className: 'keyword',
        begin: /extends/
      },
      hljs.inherit(hljs.TITLE_MODE, {
        begin: LEAN_IDENT_RE
      }),
      {
        className: 'params',
        begin: /[([{]/, end: /[)\]}]/, endsParent: false,
        keywords: LEAN_KEYWORDS,
      },
      {
        className: 'symbol',
        begin: /:/,
        endsParent: true
      },
    ],
    keywords: LEAN_KEYWORDS
  };
  return {
    name: "lean",
    keywords: LEAN_KEYWORDS,
    contains: [
      hljs.QUOTE_STRING_MODE,
      hljs.NUMBER_MODE,
      DASH_COMMENT,
      MULTI_LINE_COMMENT,
      DOC_COMMENT,
      LEAN_DEFINITION,
      ATTRIBUTE_DECORATOR,
      ATTRIBUTE_LINE,
      { begin: /‚ü®/ } // relevance booster
    ]
  };

            }}());
                //hljs.registerLanguage("lean",function(e){var r="([a-zA-Z]|\\.[a-zA-Z.])[a-zA-Z0-9._]*";return {c: [ e.HCM,e.CBCM,{b:r,l:r,k:{keyword:"maximize subject to var ",literal:""},r:90},{cN:"number",b:"\\d+(?:\\.\\d*)?(?:[eE][+\\-]?\\d*)?i?\\b",r:0}]} });

                // console.log(hljs.listLanguages());
                document.querySelectorAll('pre code').forEach((block) => { hljs.highlightElement(block); });
                //hljs.highlightAll();
            });


		</script>

    </body>
</html>
